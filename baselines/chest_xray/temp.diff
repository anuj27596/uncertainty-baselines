71a72
> flags.DEFINE_integer('num_mc_samples', default=5, help='number of Monte Carlo samples')
120,128d120
<   if config.class_reweight_mode == 'constant':  # EDIT(anuj): class weighting
<     class_weights = 0.5 * 35126 / jnp.array([28253, 6873])  # TODO(anuj): remove hardcode
<     if config.loss == 'softmax_xent':
<       base_loss_fn = train_utils.reweighted_softmax_xent(class_weights)
<     else:
<       raise NotImplementedError(f'loss `{config.loss}` not implemented for `constant` reweighting mode')
<   else:
<     base_loss_fn = getattr(train_utils, config.loss)
< 
282,285c274,290
<   def evaluation_fn(params, images, labels):
<     logits, out = model.apply(
<         {'params': flax.core.freeze(params)}, images, train=False)
<     losses = base_loss_fn(logits=logits, labels=labels, reduction=False)  # EDIT(anuj)
---
>   def evaluation_fn(params, images, labels, rng):
>     logits_list = []
>     pre_list = []
>     for _ in range(FLAGS.num_mc_samples):
>       rng, rng_model = jax.random.split(rng, 2)
>       rng_model_local = jax.random.fold_in(rng_model, jax.lax.axis_index('batch'))
>       logits, out = model.apply(
>           {'params': flax.core.freeze(params)}, images,
>           train=True, rngs={'dropout': rng_model_local})
>       logits_list.append(logits)
>       pre_list.append(out['pre_logits'])
> 
>     logits = jnp.stack(logits_list, axis=-1)
>     pre_list = jnp.stack(pre_list, axis=-2)
>     probs = jnp.mean(jax.nn.softmax(logits, axis=-2), axis=-1)
>     losses = -jnp.sum(labels * jnp.log(probs), axis=-1)
> 
287c292
<     top1_idx = jnp.argmax(logits, axis=1)
---
>     top1_idx = jnp.argmax(probs, axis=1)
295,296c300,301
<         logits, labels, out['pre_logits']], axis_name='batch')
<     return ncorrect, loss, n, metric_args
---
>         logits, labels, pre_list], axis_name='batch')
>     return ncorrect, loss, n, metric_args, rng
320c325
<       return base_loss_fn(logits=logits, labels=labels)  # EDIT(anuj)
---
>       return train_utils.softmax_xent(logits=logits, labels=labels)  # EDIT(anuj)
500a506
>             'logits': [],
508c514
<           batch_ncorrect, batch_losses, batch_n, batch_metric_args = (  # pylint: disable=unused-variable
---
>           batch_ncorrect, batch_losses, batch_n, batch_metric_args, train_loop_rngs = (  # pylint: disable=unused-variable
510c516
<                   opt_repl.target, batch['image'], batch['labels']))
---
>                   opt_repl.target, batch['image'], batch['labels'], train_loop_rngs))
522c528,529
<           probs = jax.nn.softmax(logits)
---
>           probs = jax.nn.softmax(logits, axis=-2)
>           probs = jnp.mean(probs, axis=-1)
530a538
>           results_arrs['logits'].append(logits)
538a547
>         results_arrs['logits'] = np.concatenate(results_arrs['logits'], axis=0)
